{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, *layers) -> None:\n",
    "        self.layers = self._init_all_layers(input_size, layers)\n",
    "    \n",
    "    def _init_all_layers(self, input_size: int, layers: list) -> list:\n",
    "        initialized_layers = []\n",
    "        last_output_size = input_size + 1\n",
    "        for n_neurons in layers:\n",
    "            # Create the layer weights matrix (+bias)\n",
    "            initialized_layers.append(\n",
    "                self._init_layer(last_output_size, n_neurons)\n",
    "            )\n",
    "            # Update the number of outputs in the last layer\n",
    "            last_output_size = n_neurons\n",
    "            \n",
    "        return initialized_layers\n",
    "\n",
    "    def _init_layer(self, layer_input_size: int, n_neurons: int) -> np.ndarray:\n",
    "        return np.random.uniform(\n",
    "            -1,  # from -1\n",
    "            1,  # to 1\n",
    "            (layer_input_size, n_neurons)  # col = 1 neuron\n",
    "        )\n",
    "    \n",
    "    def _normalize(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "    def preprocess(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply transformations to the data to ease learning.\n",
    "\n",
    "        Transformations include: Normalization (mean zero, std. deviation 1);\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        X_cp = X.copy()\n",
    "        for col in range(X_cp.shape[1]):\n",
    "            X_cp[..., col] = self._normalize(X_cp[..., col])\n",
    "        return X_cp\n",
    "        \n",
    "\n",
    "    def activation(self, net: np.ndarray) -> np.ndarray:\n",
    "        # Sigmoid\n",
    "        return 1. / (1. + np.exp(-net))\n",
    "\n",
    "    def _activation_derivative(self, net: np.ndarray) -> np.ndarray:\n",
    "        # Derivative of the sigmoid\n",
    "        return net*(1-net)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Do a forward pass using the inputs and weights specified.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Inputs (With or Without bias column added)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predictions\n",
    "        \"\"\"\n",
    "        # Add a column of ones at the start of the array\n",
    "        X = self._add_ones(X)\n",
    "\n",
    "        out = X\n",
    "        i=1\n",
    "        for w in self.layers:\n",
    "            # Apply weights and biases\n",
    "            net = np.dot(out, w)\n",
    "            # Apply activation function\n",
    "            out = self.activation(net)\n",
    "            i+=1\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fit(self, \n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            lr: float = 0.1,\n",
    "            epochs: int = 1,\n",
    "            batch_size: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"Fits the perceptron to the given target labels.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Inputs (Dataset)\n",
    "            y (np.ndarray): Target (Labels)\n",
    "            lr (float, optional): Learning Rate. Defaults to 0.1.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 1.\n",
    "            batch_size (int, optional): Size of each batch. Defaults to 1.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: updated weights and bias.\n",
    "        \"\"\"\n",
    "        X = self._add_ones(X)\n",
    "        batched_X = np.array_split(X,len(X)/batch_size)\n",
    "        batched_y = np.array_split(y,len(y)/batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in zip(batched_X,batched_y):\n",
    "                # Forward\n",
    "                out = X_batch\n",
    "                net_array = []\n",
    "                f_net_array = []\n",
    "                for w in self.layers:\n",
    "                    # Apply weights and biases\n",
    "                    net_array.append(np.dot(out, w))\n",
    "                    # Apply activation function\n",
    "                    f_net_array.append(self.activation(net_array[-1]))\n",
    "                    out = f_net_array[-1]\n",
    "\n",
    "                # Backpropagate\n",
    "                # Last layer\n",
    "                error = (y_batch-f_net_array[-1])*self._activation_derivative(f_net_array[-1])\n",
    "                delta_w = lr*np.dot(f_net_array[-2].T, error) / len(X)\n",
    "                self.layers[-1] += delta_w\n",
    "                # Hidden\n",
    "                e = np.dot(error, delta_w.T) * self._activation_derivative(f_net_array[-2])\n",
    "                delta_w = lr*np.dot(X_batch.T, e)\n",
    "                self.layers[-2] += delta_w\n",
    "\n",
    "                pred_class = np.argmax(out, axis=1)\n",
    "                gt_class = np.argmax(y, axis=1)\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                    f\"Acc = {np.sum(pred_class==gt_class)/len(pred_class)}\" # TODO: Fix to work with batches (works with single images)\n",
    "                )\n",
    "        return w\n",
    "\n",
    "    def _add_ones(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Add a column of ones to the start of the array \n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Target array\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array with the extra column added\n",
    "        \"\"\"\n",
    "        ones_col = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate([ones_col, X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_iris()\n",
    "\n",
    "X, y = ds['data'], ds['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "print(f'Features: {\" | \".join(ds[\"feature_names\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(4, 9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing X\n",
    "X = p.preprocess(X)\n",
    "\n",
    "# One-hot encoding (since the output will be 3 neurons)\n",
    "y = np.eye(len(np.unique(y)))[y]\n",
    "\n",
    "# Split test / training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (150,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# p.fit(X[0].reshape((1,-1)), y[0].reshape((1,-1)), 0.1, 1)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m p\u001b[38;5;241m.\u001b[39mpredict(X[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[2], line 126\u001b[0m, in \u001b[0;36mPerceptron.fit\u001b[1;34m(self, X, y, lr, epochs, batch_size)\u001b[0m\n\u001b[0;32m    122\u001b[0m         pred_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(out, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    123\u001b[0m         gt_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 126\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mpred_class\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mgt_class\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(pred_class)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m         )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (150,) "
     ]
    }
   ],
   "source": [
    "# p.fit(X[0].reshape((1,-1)), y[0].reshape((1,-1)), 0.1, 1)\n",
    "p.fit(X, y, 0.1, 50000, 10)\n",
    "\n",
    "p.predict(X[0].reshape((1,-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLENG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
